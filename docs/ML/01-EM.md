# EM 算法

通常介绍EM算法都会先介绍一个特殊的问题：如何将一堆样本拟合成某个高斯混合模型（GMM）。因为直接讲EM算法所解决的抽象问题，初学者会非常奇怪，现实中怎么会有这种形式的问题。但是我觉得从抽象的问题讲起很优雅，所以我就干脆先从抽象的问题讲起。

## 目标问题
首先，给定一个假设集 $\mathcal{H}=\{\mathcal{P}_{\theta}(x)\}$。
现在给我们从未知分布 $\mathcal{D}$ 中独立采集一堆样本  $\mathcal{S} = \{x_1, x_2, \ldots, x_m\}$，
我们希望确定哪个 $\mathcal{P}_{\theta}(x)$ 更接近这组数据的真实分布 $\mathcal{D}$ 。

那么，我们需要一个损失函数来衡量两个分布的接近程度。这里我们用的是KL距离，来构造真实误差函数：

$$
    \tag{0} \label{trueloss}
    L_{\mathcal{D}}(p_{\theta}) = \mathbb{E}_{p_\mathcal{D}(\mathbf{x})}\left[\ln\left( \frac{p_{\mathcal{D}}(\mathbf{x})}{p_{\theta}(\mathbf{x})} \right)\right].
$$


那么，对应的经验误差函数为：

$$
    L_{\mathcal{S}}(\theta) =-\frac{1}{m}\sum^{m}_{i=1} \ln \mathcal{P}_{\theta}(x_i). \tag{1}
$$

在有些情况下（高斯混合模型），$\mathcal{P}_\theta(x)$ 还和另一个 **隐随机变量** $\mathcal{Z} = \{z_1, z_2, \ldots, z_k\}$ 有关：

$$
    \mathcal{P}_\theta(x) = \sum^{k}_{j=1} \mathcal{P_{\theta}(z_j)} \mathcal{P}_\theta(x \vert z_j) 
    =\sum^k_{j=1} \mathcal{P}_\theta(x, z_j). \tag{2}
$$

当我们被限制只能获得 $\mathcal{P}_{\theta}(z)$ 和 $\mathcal{P}_\theta(x \vert z_j)$ 时， 目标函数的优化就变得非常困难了:

$$
    L_{\mathcal{S}}(\theta) = -\sum^{m}_{i=1} \ln 
    \sum^{k}_{j=1} \mathcal{P_{\theta}(x, z_j)}. \tag{3}
$$

> 这里 *概率* 还是 *概率密度* 在很多资料里非常容易搞混。

## EM算法
我们先介绍另一个损失函数，乍一看和前面问题毫无关系， 其实有非常巧妙的联系:

$$
G_{\mathcal{S}}(Q, \theta) = 
\sum^{m}_{i=1}\sum^{k}_{j=1} Q_{i, j} \ln Q_{i, j} - \sum^{m}_{i=1} \sum^{k}_{j=1} Q_{i, j} \log[ \mathcal{P}_{\theta}(x_i, z_j)]. \tag{4}
$$

EM算法的流程如下：

$$
    \begin{cases}
        \tag{5}
        Q^{(t+1)} = \min_{Q} G_{\mathcal{S}}(Q, \theta^{(t)}), \\
        \theta^{(t+1)} = \min_{\theta} G_{\mathcal{S}}(Q^{(t+1)}, \theta).
    \end{cases}
$$

要了解$G_{\mathcal{S}}(Q, \theta)$ 与 $L_{\mathcal{S}}(\theta)$ 我们需要研究优化过程中的第一步。

$$
\begin{aligned}
& G_{S}(Q, \theta) \\
=& 
-\sum^{m}_{i=1}\sum^{k}_{j=1} Q_{i, j} \ln \left\{\frac{\mathcal{P}_{\theta}(x_i, z_j)}{Q_{i, j}}\right\} \\
\ge&
-\sum^{m}_{i=1}\ln \left\{\sum^{k}_{j=1} Q_{i, j} \frac{\mathcal{P}_{\theta}(x_i, z_j)}{Q_{i, j}}\right\} \\
=&
-\sum^{m}_{i=1}\ln \left\{\sum^{k}_{j=1} \mathcal{P}_{\theta}(x_i, z_j)\right\} \\
=&L_{\mathcal{S}}(\theta)
\end{aligned}
$$

当 $Q_{i, j} = \mathcal{P}_{\theta}(z_j \vert x_i)$ 时，

$$
    \begin{aligned}
    &G_{\mathcal{S}}(Q, \theta) \\
    =&
    -\sum^{m}_{i=1} \sum^{k}_{j=1}
    \mathcal{P_{\theta}}(z_j \vert x_i) 
    \ln \left\{\frac{\mathcal{P}_{\theta}(x_i, z_j)}{\mathcal{P}_{\theta}(z_j \vert x_i)}\right\} \\
    =&
    -\sum^{m}_{i=1} \sum^{k}_{j=1}
    \mathcal{P_{\theta}}(z_j \vert x_i) 
    \ln \mathcal{P}_{\theta}(x_i) \\
    =& -\sum^{m}_{i=1} \ln \mathcal{P}_{\theta}(x_i) \\
    =& L_{\mathcal{S}}(\theta).
    \end{aligned}
$$

综上我们可以得到第一步优化存在闭解： $Q_{i,j}^{(t+1)} = \mathcal{P}_{\theta^{(t)}}(z_j \vert x_i)$, 并且$G_{\mathcal{S}}(Q^{(t+1)}, \theta^{(t)}) = L_{\mathcal{S}}(\theta^{(t)})$.
这一步是求期望，所以被称为 **Expectation Step** 。
这一步同时需要用到贝叶斯公式

$$
    Q^{(t+1)}_{i, j} = \frac{\mathcal{P}_{\theta^{(t)}}(z_j) \mathcal{P}_{\theta^{(t)}}(x_i \vert z_j)}{\sum^{k}_{j'=1}\mathcal{P}_{\theta^{(t)}}(z_{j'}) \mathcal{P}_{\theta^{(t)}}(x_i \vert z_{j'})}.
$$

又因为第二步（**Maximization Step**）最小化$\theta$时保证$G_{\mathcal{S}}(Q^{(t+1)}, \theta^{(t+1)}) \le G_{\mathcal{S}}(Q^{(t+1)}, \theta^{(t)})$, 所以我们可得

$$
    L_{\mathcal{S}}(\theta^{(t+1)}) 
    = G_{\mathcal{S}}(Q^{(t+2)}, \theta^{(t+1)}) 
    \le G_{\mathcal{S}}(Q^{(t+1)}, \theta^{(t+1)}) \\
    \le G_{\mathcal{S}}(Q^{(t+1)}, \theta^{(t)}) = L_{\mathcal{S}}(\theta^{(t)}).
$$

## 高斯混合模型
k个高斯分布的混合模型对应的hypothesis集如下：

$$
\mathcal{H} = 
\left\{
    \mathcal{P}_{\theta}(x) = \sum^{k}_{j = 1} \mathcal{P}_{\theta}(z_j) \mathcal{N}_{z_j}(\mu_{\theta}, \Sigma_{\theta})
\right\}.
$$

可以简化$\mathcal{P}_{\theta}(z_j) = \pi_j$, $\mathcal{N}_{z_j}(\mu_{\theta}, \Sigma_{\theta}) = \mathcal{N}(\mu_j, \Sigma_j)$, 那么hypothesis集就变成我们比较常见的形式：

$$
\mathcal{H} = 
\left\{
    \mathcal{P}_{\theta}(x) = \sum^{k}_{j = 1} \pi_j \mathcal{N}(\mu_{j}, \Sigma_{j})
\right\}.
$$

从这个特例我们可以感受到：$\mathcal{P}_{\theta}(z)$很容易获得;
这里的$x$是连续函数，我们也能很容易获得条件概率密度函数$p_{\theta}(x \vert z)$，我们定义一个极小值量$\Delta x$，那么 $\mathcal{P}_{\theta}(x\vert z) = p_{\theta}(x \vert z) \Delta x$.

首先是 **Expectation Step**:

$$
    Q_{i, j} 
    = \frac{\pi_j \mathcal{N}(\mu_j, \Sigma_j) \Delta x}{\sum^{k}_{j' = 1} \pi_j \mathcal{N}(\mu_j, \Sigma_j) \Delta x}
    = \frac{\pi_j \mathcal{N}(\mu_j, \Sigma_j)}{\sum^{k}_{j' = 1} \pi_j \mathcal{N}(\mu_j, \Sigma_j)}.
$$

那么，**Maximumization Step** 需要求解的问题是：

$$
\begin{aligned}
    &\max_{\theta} \sum^{m}_{i=1} \sum^{k}_{j=1} Q_{i,j} \ln \mathcal{P}(x_i, z_j) \\
    =&\max_{\theta} \sum^{m}_{i=1} \sum^{k}_{j=1} Q_{i,j} [\ln \pi_j + \ln \mathcal{N}(x_i, \mu_j, \Sigma_j) + \ln \Delta x] \\
\end{aligned}
$$

首先显然$\pi^*_j = \frac{1}{m}\sum_{i=1}^{m} Q_{i,j}$, 其次
因为

$$
\mathcal{N}(x_i, \mu_j, \Sigma_j)
= \frac{1}{(2\pi)^{D/2}}\frac{1}{\vert \Sigma \vert^{1/2}}
\exp\left\{-\frac{1}{2} (x_i - \mu_j)^T \Sigma_j^{-1} (x_i - \mu_j)\right\},
$$

所以也显然可得$\mu^*_j = \frac{\sum^{m}_{i=1} Q_{i, j} x_i}{\sum^{m}_{i=1} Q_{i, j}}$。
有点复杂的是求解$\Sigma_j$:

$$
    \max_{\Sigma_j} \sum^{m}_{i=1}\sum^{k}_{j=1} Q_{i, j}
    \left[\frac{1}{2} \ln\vert \Sigma^{-1} \vert - \frac{1}{2}(x_i - \mu^*_j)^T \Sigma^{-1}_{j} (x_i - \mu^*_j)\right].
$$

我们需要用到两个矩阵导数$\frac{\partial}{\partial A}\ln \vert A \vert = (A^{-1})^T$和$\frac{\partial}{\partial A}Tr(A^T B) = B$，先对$\Sigma^{-1}$求导可得：
$\Sigma_j = \frac{1}{\sum^{m}_{i=1} Q_{i, j}}\sum_{i=1}^m Q_{i, j} (x_i - \mu^*_j)(x_i - \mu^*_j)^T$.

非常有意思的是，K-means算法可以看成GMM算法限制在$\Sigma \rightarrow 0$的算法。
