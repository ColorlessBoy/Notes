# 强化学习论文阅读笔记

## 1. Stabilizing Off-Policy Q-Learning via Booststrapping Error Reduction

[论文链接](Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction)

这篇文章首先提出了一个非常重要的问题：现有的off-policy依旧是需要不断地和环境交互产生新的数据，那么有没有一种方法能够完全依靠已经有的数据集，而不和环境产生交互呢？（在我的理解里，这个设定被称为off-line）这篇论文的贡献就是：它认为现有的算法在off-line设定下不稳定性是由 **bootstrapping error** 产生的，所以它也提出了一个分析和降低bootstrapping error的方法，并且提出了一个叫做BEAR算法（bootstrapping error accumulation reduction）。

论文描述了一个非常有趣的现象：在off-line的设定下，即使使用的是专家产生的样本，现有的off-policy依旧无法收敛。（我对这个现象保持怀疑，要参考一下[7]，[12]和[13]文献求证一下。）好像原因非常简单：就是Q-Learning算法会求解值$\max_{a'}Q(s', a')$，但是$(s', a')$可能压根就没在数据集里出现过，导致了$Q(s', a')$的误差非常大。文章把这类动作叫做Out-of-distribution(OOD) actions。

以前的算法BCQ[13]中提出了一种方法，让学到的策略尽量和样本对应的策略接近，这样学到的策略所执行的动作就会大概率地出现在数据集中，不会出现过多的OOD动作。但是这就像模仿学习，或者行为克隆。如果数据集对应的策略是均匀分布策略这类本身就很差的策略，那么学到的策略也是一个就没有多少意义的次优解。我看到这里就非常好奇了，不可能凭空学到OOD动作的信息呀。

论文其实就是构造了一个受限的最优贝尔曼操作，限制条件就是样本集每个状态的动作分布和策略对应状态的动作分布要时刻保持接近（MMD距离要小于某个值）。更新公式如下所示：

$$
\pi_{\phi} := \max_{\pi \in \Delta_{\mathcal{S}}}
\mathbb{E}_{s\sim\mathcal{D}}\mathbb{E}_{a \sim \pi(\cdot \vert s)}
\bigg[
    \min_{j = 1,..,K} \hat Q_j(s, a)
\bigg]
\quad s.t. \quad
\mathbb{E}_{s \sim \mathcal{D}}[MMD(\mathcal{D}(s), \pi(\cdot \vert s))] 
\le \epsilon.
$$

这篇论文分析了这个受限最有贝尔曼操作会引起的误差，有很大一部分的证明直接调用了另一篇文章的结论，如果以后有兴趣可以再看。现在我更感兴趣这篇文章提到的2019年ICML提出来的BCQ算法。

```bib
@inproceedings{NEURIPS2019_c2073ffa,
 author = {Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {11784--11794},
 publisher = {Curran Associates, Inc.},
 title = {Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction},
 url = {https://proceedings.neurips.cc/paper/2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf},
 volume = {32},
 year = {2019}
}
```

## 2. Off-Policy Deep Reinforcement Learning without Exploration

[论文链接](https://arxiv.org/abs/1812.02900)

这篇文章也是要解决off-line的强化学习问题：如何从样本集来学习最优策略，而不需要与环境交互。模仿学习（Imitation Learning）需要样本集是根据专家策略中采样获得的，而off-line的强化学习问题则不要求样本集对应的策略的好坏。文章发现了一个非常奇怪的现象：再off-line的设定下，使用现有的强化学习算法会学到一个远远比样本集对应的执行策略（behavioral policy）差得多得策略。

文章认为这个现象主要是由于探索误差（extrapolation error）产生的：算法要探索到样本集里完全不包含的动作空间。由此，提出了一个Batch-Constrained deep Q-learning(BCQ)算法来解决这个问题，大致上就是限制策略的动作。 

文章说推测误差（Extrapolation Error）有三个来源：Absent Data、Model Bias（用采样的方式逼近贝尔曼操作导致的误差）和Training Mismatching（这一点我不太能理解，应该是说记忆池中$(s,a)$出现得频繁，贝尔曼误差就小。但是 **我们更希望** 当前执行当前策略时遇到的$(s, a)$越频繁，贝尔曼误差越小。我们可以通过加权重的方式来做修正，但是我们没办法解决当前策略会频繁出现的$(s,a)$在记忆池里压根没出现的情况。）。由于这些推测误差导致了当前的Off-policy算法在off-line设定下无法收敛。

本文提出的方法叫做Batch-Constrained，具体包含三个操作：

1. 最小化所选动作与样本集中动作的距离；
2. 尽量进入样本集中出现比较频繁的状态；
3. 最大化值函数。

已知记忆池$\mathcal{D} = \{(s_i, a_i, r_i, s'_i)\}$，我们构造集合$\mathcal{B} = \{(s_i, a_i): (s_i, a_i, \cdot, \cdot) \in \mathcal{D}\}$。那么BCQ算法的更新公式就是一个Batch-Constrainted贝尔曼操作：

$$
Q(s, a) \leftarrow(1-\alpha) Q(s, a)+\alpha\left(r+\gamma \underset{a^{\prime} \operatorname{s.t.}\left(s^{\prime}, a^{\prime}\right) \in \mathcal{B}}{\max } Q\left(s^{\prime}, a^{\prime}\right)\right).
$$

本文有定理证明，BCQ算法至少不会比Behavioral Policy差。备注，BCQ中将集合$\mathcal{B}$替换为了一个动作生成器（使用Conditional VAE）来确保生成出来的动作和$\mathcal{B}$比较接近，以此来增加小样本时的泛化性。

```bib
@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019}
}
```