


<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="This is a notes generated by Mcdocs.">
      
      
      
        <meta name="author" content="ColorlessBoy">
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.4.0">
    
    
      
        <title>Regression - Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.fe0cca5b.min.css">
      
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a46bcfb3.min.css">
      
      
        
        
        <meta name="theme-color" content="#3f51b5">
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
      <link rel="manifest" href="../../manifest.webmanifest" crossorigin="use-credentials">
    
    
    
      
    
    
  </head>
  
  
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="Notes" class="md-header-nav__button md-logo" aria-label="Notes">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Notes
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Regression
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header-nav__source">
        
<a href="https://github.com/colorlessboy/Notes/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ColorlessBoy/Notes
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Notes" class="md-nav__button md-logo" aria-label="Notes">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Notes
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/colorlessboy/Notes/" title="前往 GitHub 仓库" class="md-source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 00-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 01-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 000 40.81l195.61 195.6a28.86 28.86 0 0040.8 0l194.69-194.69a28.86 28.86 0 000-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    ColorlessBoy/Notes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href="../.." title="介绍" class="md-nav__link">
      介绍
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      机器学习
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="机器学习" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        机器学习
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../01-EM/" title="EM" class="md-nav__link">
      EM
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../02-SVM/" title="SVM" class="md-nav__link">
      SVM
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../03-Rademacher/" title="Rademacher" class="md-nav__link">
      Rademacher
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Regression
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 9h14V7H3v2m0 4h14v-2H3v2m0 4h14v-2H3v2m16 0h2v-2h-2v2m0-10v2h2V7h-2m0 6h2v-2h-2v2z"/></svg>
        </span>
      </label>
    
    <a href="./" title="Regression" class="md-nav__link md-nav__link--active">
      Regression
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    问题描述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    确定性的线性回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    随机性的线性回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    贝叶斯线性回归
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../05-AdaBoost/" title="AdaBoost" class="md-nav__link">
      AdaBoost
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../06-PAC/" title="PAC" class="md-nav__link">
      PAC
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../07-KalmanFilter/" title="Kalman Filter" class="md-nav__link">
      Kalman Filter
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../08-Classification/" title="Classification" class="md-nav__link">
      Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../09-PCA/" title="PCA" class="md-nav__link">
      PCA
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      算法
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M8.59 16.58L13.17 12 8.59 7.41 10 6l6 6-6 6-1.41-1.42z"/></svg>
      </span>
    </label>
    <nav class="md-nav" aria-label="算法" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
        </span>
        算法
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/01-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/" title="最长递增子序列" class="md-nav__link">
      最长递增子序列
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/02-%E5%8C%BA%E9%97%B4%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98/" title="区间调度问题" class="md-nav__link">
      区间调度问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/03-%E8%A3%B4%E8%9C%80%E5%AE%9A%E7%90%86/" title="裴蜀定理" class="md-nav__link">
      裴蜀定理
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/04-%E5%89%AA%E7%BB%B3%E5%AD%90%E9%97%AE%E9%A2%98/" title="剪绳子问题" class="md-nav__link">
      剪绳子问题
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/05-%E6%AC%A7%E6%8B%89%E5%87%BD%E6%95%B0/" title="欧拉函数" class="md-nav__link">
      欧拉函数
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/06-%E8%93%84%E6%B0%B4%E6%B1%A0%E6%8A%BD%E6%A0%B7/" title="蓄水池抽样" class="md-nav__link">
      蓄水池抽样
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/07-%E9%87%8D%E7%BC%96%E7%A0%81%E6%8A%80%E5%B7%A7/" title="重编码技巧" class="md-nav__link">
      重编码技巧
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../../Algorithms/08-%E6%94%BE%E6%9D%BE%E6%9D%A1%E4%BB%B6%E6%B3%95/" title="放松条件法" class="md-nav__link">
      放松条件法
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </span>
      目录
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    问题描述
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    确定性的线性回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    随机性的线性回归
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    贝叶斯线性回归
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/colorlessboy/Notes/edit/master/docs/ML/04-Regression.md" title="编辑此页" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                  
                
                
                <h1 id="_1">线性回归问题<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h1>
<h2 id="_2">问题描述<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<p>在实际生活中，我们能获得一系列的样本 <span><span class="MathJax_Preview">\mathcal{S} = \left\{ (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m) \right\}</span><script type="math/tex">\mathcal{S} = \left\{ (x_1, y_1), (x_2, y_2), \dots, (x_m, y_m) \right\}</script></span>。其中 样本属于某个集合 <span><span class="MathJax_Preview">z = (x, y) \in Z = X \times Y</span><script type="math/tex">z = (x, y) \in Z = X \times Y</script></span>,
并且我们认为这些样本是服从某个未知的联合分布 <span><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span>，即 <span><span class="MathJax_Preview">(x, y) \sim \mathcal{D}</span><script type="math/tex">(x, y) \sim \mathcal{D}</script></span>。 与之相关的未知条件分布 <span><span class="MathJax_Preview">P_{\mathcal{D}}(y \vert x)</span><script type="math/tex">P_{\mathcal{D}}(y \vert x)</script></span> 是我们希望获得的：当我们知道了 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 的值，我们就能知道 <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 的值最可能是多少。</p>
<p>如果 <span><span class="MathJax_Preview">y \in \mathbb{R}</span><script type="math/tex">y \in \mathbb{R}</script></span>, 那么求解 <span><span class="MathJax_Preview">P_\mathcal{D}(y\vert x)</span><script type="math/tex">P_\mathcal{D}(y\vert x)</script></span> 的问题也叫做回归问题。
通常我们构造一个假设集 <span><span class="MathJax_Preview">\mathcal{H}_{\theta} = \{P_{\theta}(y \vert x)\}</span><script type="math/tex">\mathcal{H}_{\theta} = \{P_{\theta}(y \vert x)\}</script></span>, 我们从假设集中选一个最接近 <span><span class="MathJax_Preview">P_\mathcal{D}(y \vert x)</span><script type="math/tex">P_\mathcal{D}(y \vert x)</script></span> 的集合 <span><span class="MathJax_Preview">P_{\theta^*}(y \vert x)</span><script type="math/tex">P_{\theta^*}(y \vert x)</script></span>。对应的优化目标是</p>
<div>
<div class="MathJax_Preview">
    \min_{\theta} L_{\mathcal{D}}(P_{\theta}) = \mathbb{E}_{ P_{\mathcal{D}}(x)} [Distance(P_{\mathcal{D}}(\cdot \vert x) \Vert P_{\theta}(\cdot \vert x))]. \tag{1}
</div>
<script type="math/tex; mode=display">
    \min_{\theta} L_{\mathcal{D}}(P_{\theta}) = \mathbb{E}_{ P_{\mathcal{D}}(x)} [Distance(P_{\mathcal{D}}(\cdot \vert x) \Vert P_{\theta}(\cdot \vert x))]. \tag{1}
</script>
</div>
<h2 id="_3">确定性的线性回归<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h2>
<p>我们可以简化这个问题：当我们知道了 <span><span class="MathJax_Preview">P_\mathcal{D}(y \vert x)</span><script type="math/tex">P_\mathcal{D}(y \vert x)</script></span> 的值后，给定一个 <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>, 我们通常预测 <span><span class="MathJax_Preview">y \in \arg\max_{y} P_{\mathcal{D}}(y \vert x)</span><script type="math/tex">y \in \arg\max_{y} P_{\mathcal{D}}(y \vert x)</script></span>。那么，我们不一定需要求解最接近 <span><span class="MathJax_Preview">P_\mathcal{D}(y \vert x)</span><script type="math/tex">P_\mathcal{D}(y \vert x)</script></span> 的条件分布，我们可以转而求解分布满足 <span><span class="MathJax_Preview">\arg\max_{y} P_{\theta}(y \vert x) \subset \arg\max_y P_{\mathcal{D}}(y \vert x)</span><script type="math/tex">\arg\max_{y} P_{\theta}(y \vert x) \subset \arg\max_y P_{\mathcal{D}}(y \vert x)</script></span>。
那么，我们可以构造更激进的候选集来简化问题:</p>
<div>
<div class="MathJax_Preview">
\mathcal{H}_\theta = \{P_{\theta}(y \vert \mathbf{x}) = \mathbf{1}_{\{y = h_{\theta}(\mathbf{x})\}}, h_\theta(\mathbf{x}) = \mathbf{w}^T_{\theta} \mathbf{x} + b_\theta\}. \tag{2}
</div>
<script type="math/tex; mode=display">
\mathcal{H}_\theta = \{P_{\theta}(y \vert \mathbf{x}) = \mathbf{1}_{\{y = h_{\theta}(\mathbf{x})\}}, h_\theta(\mathbf{x}) = \mathbf{w}^T_{\theta} \mathbf{x} + b_\theta\}. \tag{2}
</script>
</div>
<p>我们使用Wasserstein距离(输运花费函数为 <span><span class="MathJax_Preview">c(a, b)</span><script type="math/tex">c(a, b)</script></span>)，那么优化目标就变成了</p>
<div>
<div class="MathJax_Preview">
    \min_{\theta} L_{\mathcal{D}}(\theta) 
    = \mathbb{E}_{(x, y) \sim \mathcal{D}}[c(h_{\theta}(\mathbf{x}), y)]. \tag{3}
</div>
<script type="math/tex; mode=display">
    \min_{\theta} L_{\mathcal{D}}(\theta) 
    = \mathbb{E}_{(x, y) \sim \mathcal{D}}[c(h_{\theta}(\mathbf{x}), y)]. \tag{3}
</script>
</div>
<p>如果 <span><span class="MathJax_Preview">c(a, b) = \Vert a - b \Vert^2_2</span><script type="math/tex">c(a, b) = \Vert a - b \Vert^2_2</script></span>, 那么就是大家熟知的平方误差损失函数:</p>
<div>
<div class="MathJax_Preview">
L_{\mathcal{D}}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}}[\Vert h_{\theta}(\mathbf{x}) - y \Vert^2_2]. \tag{4}
</div>
<script type="math/tex; mode=display">
L_{\mathcal{D}}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}}[\Vert h_{\theta}(\mathbf{x}) - y \Vert^2_2]. \tag{4}
</script>
</div>
<p>对于样本集 <span><span class="MathJax_Preview">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</span><script type="math/tex">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</script></span>, 对应的经验误差函数为 </p>
<div>
<div class="MathJax_Preview">
L_{S}(\theta) = \frac{1}{m} \sum^{m}_{i = 1} \Vert h_{\theta}(\mathbf{x}_i) - y_i \Vert^2_2. \tag{5}
</div>
<script type="math/tex; mode=display">
L_{S}(\theta) = \frac{1}{m} \sum^{m}_{i = 1} \Vert h_{\theta}(\mathbf{x}_i) - y_i \Vert^2_2. \tag{5}
</script>
</div>
<blockquote>
<p>注意：我没有提到高斯分布，而且不需要提到高斯分布的假设。</p>
</blockquote>
<h2 id="_4">随机性的线性回归<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h2>
<p>如果我们希望不要那么激进，那么可以构造其他假设集, 例如：</p>
<div>
<div class="MathJax_Preview">
\mathcal{H}_{\theta} = \{p_{\theta}(y \vert \mathbf{x}) \sim \mathcal{N}(\pmb{\mu}^T_{\theta} \mathbf{x}, \mathbf{x}^T\pmb{\Sigma}_{\theta}\mathbf{x})\}. \tag{6}
</div>
<script type="math/tex; mode=display">
\mathcal{H}_{\theta} = \{p_{\theta}(y \vert \mathbf{x}) \sim \mathcal{N}(\pmb{\mu}^T_{\theta} \mathbf{x}, \mathbf{x}^T\pmb{\Sigma}_{\theta}\mathbf{x})\}. \tag{6}
</script>
</div>
<p>这个集合怎么来的呢？其实和贝叶斯线性回归有关系。贝叶斯线性回归就是给参数 <span><span class="MathJax_Preview">\mathbf{w}</span><script type="math/tex">\mathbf{w}</script></span> (b被包含在 <span><span class="MathJax_Preview">\mathbf{w}</span><script type="math/tex">\mathbf{w}</script></span> 中)添加一个先验分布，通常给 <span><span class="MathJax_Preview">\mathbf{w}</span><script type="math/tex">\mathbf{w}</script></span> 先验分布为某个参数化的高斯分布 <span><span class="MathJax_Preview">\mathcal{N}(\pmb{\mu}_\theta, \pmb{\Sigma}_{\theta})</span><script type="math/tex">\mathcal{N}(\pmb{\mu}_\theta, \pmb{\Sigma}_{\theta})</script></span>，即假设集为 </p>
<div>
<div class="MathJax_Preview">
\mathcal{H}_{\theta} = \{h(\mathbf{x}) = \mathbf{w}^T \mathbf{x}, \mathbf{w} \sim \mathcal{N}(\pmb{\mu}_{\theta}, \pmb{\Sigma}_{\theta})\}.\tag{7} \label{linearhypothesis}
</div>
<script type="math/tex; mode=display">
\mathcal{H}_{\theta} = \{h(\mathbf{x}) = \mathbf{w}^T \mathbf{x}, \mathbf{w} \sim \mathcal{N}(\pmb{\mu}_{\theta}, \pmb{\Sigma}_{\theta})\}.\tag{7} \label{linearhypothesis}
</script>
</div>
<p>那么，<span><span class="MathJax_Preview">\mathbb{E}_{\mathbf{w}}[h(\mathbf{x})] = \mathbf{w}^T \pmb{\mu}_{\theta}</span><script type="math/tex">\mathbb{E}_{\mathbf{w}}[h(\mathbf{x})] = \mathbf{w}^T \pmb{\mu}_{\theta}</script></span>, 并且 <span><span class="MathJax_Preview">Var_{\mathbf{w}}(h(\mathbf{x})) = \mathbf{x}^T \pmb{\Sigma_{\theta}} \mathbf{x}</span><script type="math/tex">Var_{\mathbf{w}}(h(\mathbf{x})) = \mathbf{x}^T \pmb{\Sigma_{\theta}} \mathbf{x}</script></span>。</p>
<p>这时，损失函数是有点麻烦了:</p>
<div>
<div class="MathJax_Preview">
    L_{\mathcal{D}}(\theta) = \mathbb{E}_{p_{\mathcal{D}}(\mathbf{x})}[Distance(p_{\mathcal{D}}(\cdot \vert \mathbf{x})\Vert p_{\theta}(\cdot \Vert \mathbf{x}))]. \tag{8}
</div>
<script type="math/tex; mode=display">
    L_{\mathcal{D}}(\theta) = \mathbb{E}_{p_{\mathcal{D}}(\mathbf{x})}[Distance(p_{\mathcal{D}}(\cdot \vert \mathbf{x})\Vert p_{\theta}(\cdot \Vert \mathbf{x}))]. \tag{8}
</script>
</div>
<p>在这种情况下什么距离函数可以求呢？那就是Kullback-Leibler距离：</p>
<div>
<div class="MathJax_Preview">
    L_{\mathcal{D}}(\theta) = \mathbb{E}_{p_{\mathcal{D}}(\mathbf{x})}\mathbb{E}_{p_{\mathcal{D}}(y \vert \mathbf{x})}\left[\ln\frac{p_{\mathcal{D}}(y \vert \mathbf{x})}{p_{\theta}(y \vert \mathbf{x})}\right], \tag{9}
</div>
<script type="math/tex; mode=display">
    L_{\mathcal{D}}(\theta) = \mathbb{E}_{p_{\mathcal{D}}(\mathbf{x})}\mathbb{E}_{p_{\mathcal{D}}(y \vert \mathbf{x})}\left[\ln\frac{p_{\mathcal{D}}(y \vert \mathbf{x})}{p_{\theta}(y \vert \mathbf{x})}\right], \tag{9}
</script>
</div>
<p>我们略去与 <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 无关的项，那么对应的简化过的损失函数就是交叉熵：</p>
<div>
<div class="MathJax_Preview">
    L_{\mathcal{D}}(\theta) = -\mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}}\left[\ln {p_{\theta}(y \vert \mathbf{x})}\right]. \tag{9}
</div>
<script type="math/tex; mode=display">
    L_{\mathcal{D}}(\theta) = -\mathbb{E}_{(\mathbf{x}, y) \sim \mathcal{D}}\left[\ln {p_{\theta}(y \vert \mathbf{x})}\right]. \tag{9}
</script>
</div>
<p>对于样本集 <span><span class="MathJax_Preview">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</span><script type="math/tex">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</script></span>, 对应的经验误差函数为 </p>
<div>
<div class="MathJax_Preview">
    L_{S}(\theta) = - \sum^{m}_{i=1} \ln p_{\theta}(y_i \vert \mathbf{x}_i), \tag{10}
</div>
<script type="math/tex; mode=display">
    L_{S}(\theta) = - \sum^{m}_{i=1} \ln p_{\theta}(y_i \vert \mathbf{x}_i), \tag{10}
</script>
</div>
<p>带入高斯分布的概率密度公式可得：</p>
<div>
<div class="MathJax_Preview">
\begin{align*}
    &amp;L_{S}(\theta) = -\sum^{m}_{i=1}
    \ln\left(\frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{1}{2\sigma_i^2} (y_i - \bar{y}_i)^2\right\}\right)\\
    =&amp; \sum^{m}_{i=1} \ln \sigma_i + \frac{1}{2\sigma_i^2} (y_i - \bar{y}_i)^2 + const \\
    =&amp; \sum^{m}_{i=1}\left[ \frac{1}{2}\ln(\mathbf{x}_i^T \pmb{\Sigma}_{\theta} \mathbf{x}_i) + \frac{(y_i - \mathbf{x}^T_i \pmb{\mu}_{\theta})^2}{2\mathbf{x}_i^T \pmb{\Sigma}_{\theta} \mathbf{x}_i}\right] + const.
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
    &L_{S}(\theta) = -\sum^{m}_{i=1}
    \ln\left(\frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{1}{2\sigma_i^2} (y_i - \bar{y}_i)^2\right\}\right)\\
    =& \sum^{m}_{i=1} \ln \sigma_i + \frac{1}{2\sigma_i^2} (y_i - \bar{y}_i)^2 + const \\
    =& \sum^{m}_{i=1}\left[ \frac{1}{2}\ln(\mathbf{x}_i^T \pmb{\Sigma}_{\theta} \mathbf{x}_i) + \frac{(y_i - \mathbf{x}^T_i \pmb{\mu}_{\theta})^2}{2\mathbf{x}_i^T \pmb{\Sigma}_{\theta} \mathbf{x}_i}\right] + const.
\end{align*}
</script>
</div>
<p>经验公式也可以写成</p>
<div>
<div class="MathJax_Preview">
L_S(\pmb{\mu}, \pmb{\Sigma})
    = \sum^{m}_{i=1} \left[\frac{1}{2}\ln(\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i) + \frac{(y_i - \mathbf{x}^T_i \pmb{\mu})^2}{2\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i}\right].
</div>
<script type="math/tex; mode=display">
L_S(\pmb{\mu}, \pmb{\Sigma})
    = \sum^{m}_{i=1} \left[\frac{1}{2}\ln(\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i) + \frac{(y_i - \mathbf{x}^T_i \pmb{\mu})^2}{2\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i}\right].
</script>
</div>
<p>求解：</p>
<div>
<div class="MathJax_Preview">
\begin{align*}
&amp;\nabla_{\pmb{\mu}} L_S(\pmb{\mu}, \pmb{\Sigma})
= \sum^m_{i=1} \frac{(y_i - \mathbf{x}^T_i \pmb{\mu})\mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i} \\
\Rightarrow&amp; \sum^m_{i=1} \frac{y_i \mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i} 
= \sum^m_{i=1} \frac{\mathbf{x}_i\mathbf{x}^T_i }{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i} \pmb{\mu}^* \\
\Rightarrow&amp; \pmb{\mu}^* = 
\left(\sum^m_{i=1} \frac{\mathbf{x}_i\mathbf{x}^T_i }{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i}\right)^{-1} 
\sum^m_{i=1} \frac{y_i \mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i}.
\end{align*}
</div>
<script type="math/tex; mode=display">
\begin{align*}
&\nabla_{\pmb{\mu}} L_S(\pmb{\mu}, \pmb{\Sigma})
= \sum^m_{i=1} \frac{(y_i - \mathbf{x}^T_i \pmb{\mu})\mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma} \mathbf{x}_i} \\
\Rightarrow& \sum^m_{i=1} \frac{y_i \mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i} 
= \sum^m_{i=1} \frac{\mathbf{x}_i\mathbf{x}^T_i }{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i} \pmb{\mu}^* \\
\Rightarrow& \pmb{\mu}^* = 
\left(\sum^m_{i=1} \frac{\mathbf{x}_i\mathbf{x}^T_i }{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i}\right)^{-1} 
\sum^m_{i=1} \frac{y_i \mathbf{x}_i}{\mathbf{x}_i^T \pmb{\Sigma}^* \mathbf{x}_i}.
\end{align*}
</script>
</div>
<h2 id="_5">贝叶斯线性回归<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h2>
<p>贝叶斯线性回归的假设集正如上面<span><span class="MathJax_Preview">\eqref{linearhypothesis}</span><script type="math/tex">\eqref{linearhypothesis}</script></span>式所描述的。
不过贝叶斯线性回归做的是在我们求出最优的 <span><span class="MathJax_Preview">\mathbf{w} \sim \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</span><script type="math/tex">\mathbf{w} \sim \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</script></span> 后，
我们怎么样能做得更好。当然 <span><span class="MathJax_Preview">\mathbf{w} \sim \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</span><script type="math/tex">\mathbf{w} \sim \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</script></span> 通常直接被认为给定一个分布，
人们叫它先验知识。我觉得加入上一节 <a href="#_4">随机线性回归</a> 的先验模型学习更合理。</p>
<p>我们在贝叶斯回归时，实际上是在做什么？</p>
<p>现在我们已经有了分布 <span><span class="MathJax_Preview">p(\mathbf{w})</span><script type="math/tex">p(\mathbf{w})</script></span> 和 <span><span class="MathJax_Preview">p(y \vert \mathbf{x}, \mathbf{w})</span><script type="math/tex">p(y \vert \mathbf{x}, \mathbf{w})</script></span>，在贝叶斯框架中相当于我们已经获得了全部的
和概率相关的信息。现在我们给定一个样本 <span><span class="MathJax_Preview">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</span><script type="math/tex">S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), \ldots, (\mathbf{x}_m, y_m)\}</script></span>, 
我们希望求得条件概率 <span><span class="MathJax_Preview">p(\mathbf{w} \vert S)</span><script type="math/tex">p(\mathbf{w} \vert S)</script></span> , 来指导我们选择参数 <span><span class="MathJax_Preview">\mathbf{w}</span><script type="math/tex">\mathbf{w}</script></span>。</p>
<p>令 <span><span class="MathJax_Preview">\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m]</span><script type="math/tex">\mathbf{X} = [\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m]</script></span>, <span><span class="MathJax_Preview">\mathbf{y} = [y_1, y_2, \ldots, y_m]^T</span><script type="math/tex">\mathbf{y} = [y_1, y_2, \ldots, y_m]^T</script></span>, 我们已知</p>
<div>
<div class="MathJax_Preview">\begin{cases}
    \tag{11} \label{step1}
    p(\mathbf{w}) = \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)\\
    p({\mathbf{y}} \vert \mathbf{X}, \mathbf{w}) = \mathcal{N}(\mathbf{X}^T \mathbf{w}, \mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X})
\end{cases}</div>
<script type="math/tex; mode=display">\begin{cases}
    \tag{11} \label{step1}
    p(\mathbf{w}) = \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)\\
    p({\mathbf{y}} \vert \mathbf{X}, \mathbf{w}) = \mathcal{N}(\mathbf{X}^T \mathbf{w}, \mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X})
\end{cases}</script>
</div>
<p>要求解 <span><span class="MathJax_Preview">p(\mathbf{w} \vert \mathbf{X}, \mathbf{y})</span><script type="math/tex">p(\mathbf{w} \vert \mathbf{X}, \mathbf{y})</script></span>, 我们需要引入下面这个结论：</p>
<blockquote>
<p><strong>多元高斯分布的边缘分布与条件分布</strong>：</p>
<p>已知分布：</p>
<div>
<div class="MathJax_Preview">\begin{cases}
    \tag{12} \label{step3}
    p(\mathbf{x}) = \mathcal{N}(\pmb{\mu}, \pmb{\Lambda}^{-1}), \\
    p(\mathbf{y} \vert \mathbf{x}) = \mathcal{N}(\mathbf{A}\mathbf{x} + \mathbf{b}, \pmb{L}^{-1}),
\end{cases}</div>
<script type="math/tex; mode=display">\begin{cases}
    \tag{12} \label{step3}
    p(\mathbf{x}) = \mathcal{N}(\pmb{\mu}, \pmb{\Lambda}^{-1}), \\
    p(\mathbf{y} \vert \mathbf{x}) = \mathcal{N}(\mathbf{A}\mathbf{x} + \mathbf{b}, \pmb{L}^{-1}),
\end{cases}</script>
</div>
<p>我们可得：</p>
<div>
<div class="MathJax_Preview">\begin{cases}
    \tag{13} \label{step4}
    p(\mathbf{y}) = \mathcal{N}(\mathbf{A}\pmb{\mu} + \mathbf{b}, \mathbf{L}^{-1} + \mathbf{A}\pmb{\Lambda}^{-1}\mathbf{A}^T), \\
    p(\mathbf{x} \vert \mathbf{y}) = \mathcal{N}(\pmb{\Sigma}[\mathbf{A}^T \mathbf{L}(\mathbf{y} - \mathbf{b}) + \pmb{\Lambda\mu}], \pmb{\Sigma}),\\
     \pmb{\Sigma} = (\pmb{\Lambda} + \mathbf{A}^T \mathbf{L} \mathbf{A})^{-1}.
\end{cases}</div>
<script type="math/tex; mode=display">\begin{cases}
    \tag{13} \label{step4}
    p(\mathbf{y}) = \mathcal{N}(\mathbf{A}\pmb{\mu} + \mathbf{b}, \mathbf{L}^{-1} + \mathbf{A}\pmb{\Lambda}^{-1}\mathbf{A}^T), \\
    p(\mathbf{x} \vert \mathbf{y}) = \mathcal{N}(\pmb{\Sigma}[\mathbf{A}^T \mathbf{L}(\mathbf{y} - \mathbf{b}) + \pmb{\Lambda\mu}], \pmb{\Sigma}),\\
     \pmb{\Sigma} = (\pmb{\Lambda} + \mathbf{A}^T \mathbf{L} \mathbf{A})^{-1}.
\end{cases}</script>
</div>
<p>(参考自：PRML(2006年出版)第93页)</p>
</blockquote>
<p>带入上面的结论，我们可得：</p>
<div>
<div class="MathJax_Preview">
\tag{14} \label{step5}
p(\mathbf{w} \vert {\mathbf{y}}, \mathbf{X}) = \mathcal{N}(\pmb{\mu}, \pmb{\Sigma}),
</div>
<script type="math/tex; mode=display">
\tag{14} \label{step5}
p(\mathbf{w} \vert {\mathbf{y}}, \mathbf{X}) = \mathcal{N}(\pmb{\mu}, \pmb{\Sigma}),
</script>
</div>
<p>其中 <span><span class="MathJax_Preview">\pmb{\Sigma} = (\pmb{\Sigma}_0 + \mathbf{X} \mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X}\mathbf{X}^T)^{-1}</span><script type="math/tex">\pmb{\Sigma} = (\pmb{\Sigma}_0 + \mathbf{X} \mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X}\mathbf{X}^T)^{-1}</script></span>，
并且 <span><span class="MathJax_Preview">\pmb{\mu} = \pmb{\Sigma} [\mathbf{X}(\mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X})^{-1} \mathbf{y} + \pmb{\Sigma}_0^{-1} \pmb{\mu}_0]</span><script type="math/tex">\pmb{\mu} = \pmb{\Sigma} [\mathbf{X}(\mathbf{X}^T \pmb{\Sigma}_0 \mathbf{X})^{-1} \mathbf{y} + \pmb{\Sigma}_0^{-1} \pmb{\mu}_0]</script></span>。</p>
<p>这个公式还不够简练，求逆的操作比较多，我参考了<a href="../07-KalmanFilter/">卡尔曼滤波</a>的公式来化简这个求解。</p>
<blockquote>
<p>我抽象了一下卡尔曼滤波中的一个结论。
已知分布<span><span class="MathJax_Preview">p(\mathbf{x}) = \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</span><script type="math/tex">p(\mathbf{x}) = \mathcal{N}(\pmb{\mu}_0, \pmb{\Sigma}_0)</script></span> 和 <span><span class="MathJax_Preview">p(\mathbf{y} \vert \mathbf{x}) = \mathcal{N}(\mathbf{A}\mathbf{x} + \mathbf{b}, \mathbf{C})</span><script type="math/tex">p(\mathbf{y} \vert \mathbf{x}) = \mathcal{N}(\mathbf{A}\mathbf{x} + \mathbf{b}, \mathbf{C})</script></span>，我们可得: <span><span class="MathJax_Preview">p(\mathbf{x} \vert \mathbf{y}) = \mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)</span><script type="math/tex">p(\mathbf{x} \vert \mathbf{y}) = \mathcal{N}(\pmb{\mu}_1, \pmb{\Sigma}_1)</script></span>，满足</p>
<div>
<div class="MathJax_Preview">\begin{cases}
    \tag{15} \label{kalmanfilter}
    \mathbf{K} = \pmb{\Sigma}_0 \mathbf{A}^T (\mathbf{A} \pmb{\Sigma}_0 \mathbf{A}^T + \mathbf{C})^{-1};\\
    \pmb{\Sigma}_1 = (\mathbf{I} - \mathbf{K} \mathbf{A}) \mathbf{\Sigma}_0;\\
    \pmb{\mu}_1 = \pmb{\mu}_0 + \mathbf{K} [(\mathbf{y} - \mathbf{b}) - \mathbf{A} \pmb{\mu}_0]
\end{cases}</div>
<script type="math/tex; mode=display">\begin{cases}
    \tag{15} \label{kalmanfilter}
    \mathbf{K} = \pmb{\Sigma}_0 \mathbf{A}^T (\mathbf{A} \pmb{\Sigma}_0 \mathbf{A}^T + \mathbf{C})^{-1};\\
    \pmb{\Sigma}_1 = (\mathbf{I} - \mathbf{K} \mathbf{A}) \mathbf{\Sigma}_0;\\
    \pmb{\mu}_1 = \pmb{\mu}_0 + \mathbf{K} [(\mathbf{y} - \mathbf{b}) - \mathbf{A} \pmb{\mu}_0]
\end{cases}</script>
</div>
</blockquote>
<p>最后，我们可以获得贝叶斯线性回归的求解公式：</p>
<div>
<div class="MathJax_Preview">\begin{cases}
    \tag{16} \label{step6}
    \pmb{K} = {\pmb{\Sigma}}_0 \mathbf{X}(2 \mathbf{X}^{T} {\pmb{\Sigma}}_0 \mathbf{X})^{-1};\\
    \pmb{\Sigma} = (\mathbf{I} - \mathbf{K} \mathbf{X}^T) {\mathbf{\Sigma}}_0;\\
    \pmb{\mu} = {\pmb{\mu}}_{0} + \mathbf{K}[\mathbf{y}  - \mathbf{X}^{T} {\pmb{\mu}}_0]. \\
\end{cases}</div>
<script type="math/tex; mode=display">\begin{cases}
    \tag{16} \label{step6}
    \pmb{K} = {\pmb{\Sigma}}_0 \mathbf{X}(2 \mathbf{X}^{T} {\pmb{\Sigma}}_0 \mathbf{X})^{-1};\\
    \pmb{\Sigma} = (\mathbf{I} - \mathbf{K} \mathbf{X}^T) {\mathbf{\Sigma}}_0;\\
    \pmb{\mu} = {\pmb{\mu}}_{0} + \mathbf{K}[\mathbf{y}  - \mathbf{X}^{T} {\pmb{\mu}}_0]. \\
\end{cases}</script>
</div>
                
                  
                    

<hr>
<div class="md-source-date">
  <small>
    
      最后更新: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date">2020年7月17日</span>
    
  </small>
</div>
                  
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../03-Rademacher/" title="Rademacher" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  上一页
                </span>
                Rademacher
              </div>
            </div>
          </a>
        
        
          <a href="../05-AdaBoost/" title="AdaBoost" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  下一页
                </span>
                AdaBoost
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.d710d30a.min.js"></script>
      <script src="../../assets/javascripts/bundle.b39636ac.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "\u590d\u5236", "clipboard.copied": "\u5df2\u590d\u5236", "search.config.lang": "ja", "search.config.pipeline": "trimmer, stemmer", "search.config.separator": "[\\uff0c\\u3002]+", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.a68abb33.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>